<!DOCTYPE html>
<html lang="en-us">
    
  <head>
    <link href="https://gmpg.org/xfn/11" rel="profile">
    <meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta http-equiv="Cache-Control" content="public" />
<!-- Enable responsiveness on mobile devices -->
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">
<meta name="generator" content="Hugo 0.46" />

    

<title></title>


<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content=""/>
<meta name="twitter:description" content="Yingyan Xu"/>

<meta property="og:title" content="" />
<meta property="og:description" content="Yingyan Xu" />
<meta property="og:type" content="website" />
<meta property="og:url" content="https://yingyan-xu.github.io/" />



<meta property="og:updated_time" content="2018-08-13T23:52:44&#43;08:00"/>

<meta property="og:site_name" content="Yingyan Xu" />










    

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">



<link rel="stylesheet" href="/css/hyde-hyde.css">
<link rel="stylesheet" href="/css/print.min.css" media="print">

    <!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
    <script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
    <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
    <![endif]-->
    <!-- Icons -->
    <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
    
    

<link href="https://yingyan-xu.github.io/index.xml" rel="alternate" type="application/rss+xml" title="" />
<link href="https://yingyan-xu.github.io/index.xml" rel="feed" type="application/rss+xml" title="" />


</head>


    <body class="theme-base-08 ">
        
<div class="sidebar">
  <div class="container ">
    <div class="sidebar-about">
      <span class="site__title">
        <a href="https://yingyan-xu.github.io/"></a>
      </span>
      
      
      
      <div class="author-image">
        <img src="https://yingyan-xu.github.io/img/HEAD.jpg" alt="Author Image" class="img--circle img--headshot element--center"> 
      </div>
      
      <p class="site__description">
         Yingyan Xu 
      </p>
    </div>
    <div>
	<ul class="sidebar-nav">
		 
		 
			 
				<li>
					<a href="#about">
						<span>About</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="#education">
						<span>Education</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="#internship">
						<span>Internship</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="#projects">
						<span>Projects</span>
					</a>
				</li>
			 
		 
			 
				<li>
					<a href="#coursework">
						<span>Coursework</span>
					</a>
				</li>
			 
		
		</li>
	</ul>
</div>

    <p>
      <section class="social">
	
	
	
	
	
	
	
	
	
	
	
	
	&nbsp;<a href="mailto:yingyanxu.0102@gmail.com"><i class="fas fa-at fa-lg" aria-hidden="true"></i></a>
	
</section>

    </p>
    
  </div>
  <div>
  </div>
</div>

        <div class="content container">
            
  <div class="post-list">
    
    
      <div class="post-list__item">
        <span class="item__title--big">
          <a>ABOUT</a>
        </span>
        
        <p>I am a third year undergraduate student in Department of Automation, <a href="http://en.sjtu.edu.cn/">Shanghai Jiao Tong University</a>, expecting to graduate in June 2019.</p>

<p>My research interests include <strong>video analysis</strong>, <strong>robotic vision</strong> and general problems in <strong>computer vision</strong>. My current work focuses on dense detailed multi-label <strong>action detection</strong> tasks in untrimmed videos.</p>

<hr />

        
        </div>
        
      <div class="post-list__item">
        <span class="item__title--big">
          <a>EDUCATION</a>
        </span>
        
        <p><font size=5>SHANGHAI JIAO TONG UNIVERSITY</font></br></p>

<ul>
<li>Major in Automation, School of Electronic Information and Electrical Engineering</li>
<li>Average Score: 90.4 / 100</li>
<li>Rank: 3 / 91<br /></li>
</ul>

<hr />

        
        </div>
        
      <div class="post-list__item">
        <span class="item__title--big">
          <a>INTERNSHIP</a>
        </span>
        
        <p><font size=5>HIKVISION RESEARCH INSTITUTE</font></br>
<font color="gray">Research Intern, Intelligent Algorithm Department</font></p>

<p>I’ve recently been researching on visual analysis of human activity and have done some related work, listed as follows:</p>

<ul>
<li><p>Examined large-scale video benchmark datasets which depict a variety of complex human activities in untrimmed videos, including <strong>Charades</strong>, <strong>THUMOS</strong> and <strong>MultiTHUMOS</strong>.</p></li>

<li><p>Learned the basic ideas of existing approaches on MultiTHUMOS, including <strong>MultiLSTM</strong>, <strong>predictive-corrective networks</strong>, <strong>biLSTM</strong>, <strong>siLSTM</strong> and <strong>super-event</strong> presentation. Briefly compared their strengths and weaknesses. Summarized their respective innovations and contributions.</p></li>

<li><p>Focused on modeling temporal relationships between activities. Thoroughly studied the algorithm details of <strong>sub-event</strong>, <strong>super-event</strong> and the latest <strong>TGM</strong> (Temporal Gaussian Mixture) approach, which obtained the best known performance. I&rsquo;ve been carrying out experiments after code implementation and have gained some results similar to the authors’ (<strong>34.4 mAP</strong> for I3D+super-event).</p></li>
</ul>

<p>Future works may include:</p>

<ul>
<li>Visualization works to explicitly demonstrate what the learned sub-events and super-events are.<br /></li>
<li>Analyzing correct and incorrect detection examples to gain insights and further improve the algorithms.<br /></li>
<li>Combination of multiple existing approaches to generate new ideas.<br /></li>
<li>Reaching out to other datasets and approaches for other tasks in video analysis area, such as classification and captioning.</li>
</ul>

<hr />

        
        
        
        </div>
        
      <div class="post-list__item">
        <span class="item__title--big">
          <a>PROJECTS</a>
        </span>
        
        <p><font size=5>Obstacle Detection for Vision Based Navigation for Autonomous Driving</font></br>
<a href="http://robotics.sjtu.edu.cn/index.php?r=site/index">Autonomous Robot Lab, SJTU</a></p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="//www.youtube.com/embed/4-pSmm8NlNI" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>Detecting the obstacles accurately and reliably is important for navigating the robot or autonomous vehicle. The cost effective monocular camera sensor along with computer vision techniques plays a key role in making this pragmatically realizable. Deep Neural Networks like Convolutional Neural Network (CNN) are widely used for object detection and segmentation algorithms.</p>

<ul>
<li><p>Used the <strong>SegNet</strong> encoder-decoder architecture for pixel-wise <strong>semantic segmentation</strong> of the video frame. The network was trained and tested using CamVid dataset, achieving a test accuracy of <strong>68.4%</strong>.</p></li>

<li><p>For road region <strong>contour detection</strong> step, the <strong>simple chain approximation</strong> method helped reduce the redundancy by saving only the essential points of the contour instead of storing every single point that constitutes the contour.</p></li>

<li><p>Used the <strong>convex hull</strong> instead of just using the contour in order to avoid missing out any obstacles that might appear very closer or partially over the road region.</p></li>

<li><p>Plotted minimum fitting bounding rectangles over the vehicle and pedestrian regions using <strong>bounding box</strong> algorithm. Used the <strong>point polygon test</strong> to determine whether an obstacle (pedestrians or other vehicles) is inside the ROI, i.e., within the convex hull of the road region.</p></li>
</ul>

<p>Future work can include integrating this methodology to the autonomous platform available in the campus, e.g., the logistic robot in our lab.
&nbsp;</p>

<p>&nbsp;</p>

<p>&nbsp;</p>

<p><font size=5>Object Counting</font></br>
<font color="gray">Digital Image Processing Course</font></p>

<p><img src="https://raw.githubusercontent.com/Yingyan-Xu/Yingyan-Xu.github.io/master/dip.PNG" alt="Aaron Swartz" />
The number of objects in an image is one of the basic information and is of great use. The tedious counting process is time consuming and should be replaced by machines. I tried out counting tasks for common block objects (e.g., apples, coins) using <strong>watershed segmentation</strong> and strip objects (e.g., matches, cigarettes) based on <strong>Hough line transform</strong>, then focused on automatic segmentation and counting in cell images and implemented a method as follows:</p>

<ul>
<li><p>Applied a <strong>threshold filter</strong> and <strong>morphological transform</strong> to reduce the noise. Detected the adhere cells based on the area and shape of the cells.</p></li>

<li><p>The boundary information was used to generate the <strong>Freeman chain codes</strong>.</p></li>

<li><p>Identified the <strong>concave points</strong> based on the relationship between the difference of the chain code and the curvature. Cell segmentation and counting were completed based on the characteristics of the concave points.
&nbsp;</p></li>
</ul>

<p>&nbsp;</p>

<p>&nbsp;</p>

<p><font size=5>Smart Car Projects – Vision Based and Sensor Based</font></br>
<font color="gray">Intelligent Mobile Robot Project</font></p>


<div style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;">
  <iframe src="//www.youtube.com/embed/dRoUKyuKtCY" style="position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;" allowfullscreen title="YouTube Video"></iframe>
</div>


<p>For a Raspberry Pi car robot with only camera sensors, one in the front and another in the back, implemented functions as follows:</p>

<ul>
<li>Automatic line following.</br></li>
<li>Stop according to the stop sign.</br></li>
<li>Reverse parking.</br></li>
<li>Move forward following the arrow signs.<br /></li>
</ul>

<p>For a sensor based car robot equipped with three sensor modules including infrared distance sensors, line followers and collision sensors, implemented functions as follows:</p>

<ul>
<li>Automatic line following.</br></li>
<li>Solve a maze.</br></li>
<li>Remote control with Bluetooth.</br></li>
</ul>

<hr />

        
        
        
        </div>
        
      <div class="post-list__item">
        <span class="item__title--big">
          <a>COURSEWORK</a>
        </span>
        
        

<h4 id="upcoming-courses">UPCOMING COURSES</h4>

<table>
   <tr>
      <td>AU419</td>
      <td>Computer Networking</td>
      <td>Fall 2018</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU422</td>
      <td>Operation Systems</td>
      <td>Fall 2018</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>CS400</td>
      <td>Database</td>
      <td>Fall 2018</td>
      <td>SJTU</td>
   </tr>
</table>

<h4 id="completed-courses">COMPLETED COURSES</h4>

<table>
   <tr>
      <td>CS430</td>
      <td>Data Structure</td>
      <td>100</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>CS048</td>
      <td>C++ Programing</td>
      <td>98</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>CS331</td>
      <td>Computer Control Technology </td>
      <td>100</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU416</td>
      <td>Robotics</td>
      <td>97</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU310</td>
      <td>Motion Control Systems</td>
      <td>95</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU326</td>
      <td>Digital Image Processing</td>
      <td>90</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU311</td>
      <td>Pattern Recognition</td>
      <td>90</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>EI304</td>
      <td>Principles of Automatic Control</td>
      <td>92</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>MA119</td>
      <td>Probability and Statistics</td>
      <td>90</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>MA043</td>
      <td>Mathematical Analysis</td>
      <td>93</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>MA115</td>
      <td>Discrete Mathematics</td>
      <td>88</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>MA077</td>
      <td>Linear Algebra</td>
      <td>87</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU302</td>
      <td>Modern Control Theory</td>
      <td>98</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>AU308</td>
      <td>Digital Signal Processing</td>
      <td>97</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>EE213</td>
      <td>Embedded Systems</td>
      <td>98</td>
      <td>SJTU</td>
   </tr>
   <tr>
      <td>PH002</td>
      <td>Physics</td>
      <td>97</td>
      <td>SJTU</td>
   </tr>
</table>

<h4 id="online-courses">ONLINE COURSES</h4>

<table>
   <tr>
      <td>Deep Learning</td>
      <td>Summer 2018</td>
      <td>Coursera</td>
   </tr>
</table>

        
        </div>
        
     
  </div>

        </div>
        

  
  
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-123995888-2', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script defer src="https://use.fontawesome.com/releases/v5.2.0/js/all.js"
  integrity="sha384-4oV5EgaV02iISL2ban6c/RmotsABqE4yZxZLcYMAdG7FAPsyHYAPpywE9PJo+Khy"
  crossorigin="anonymous">
</script>


    </body>
</html>
